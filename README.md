# LRM-Faithfulness-under-Adversarial-Injection
Evaluating and enhancing the faithfulness of Large Reasoning Models (LRMs) to system prompts under adversarial prompt injection attacks. 
